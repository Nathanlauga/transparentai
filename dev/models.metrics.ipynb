{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def true_positives(y_true, y_pred, num_label=1):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    if type(y_true) == list:\n",
    "        y_true = np.array(y_true)\n",
    "    if type(y_pred) == list:\n",
    "        y_pred = np.array(y_pred)\n",
    "    \n",
    "    return np.sum((y_true == num_label) & (y_pred == num_label))\n",
    "\n",
    "def false_positives(y_true, y_pred, num_label=1):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    if type(y_true) == list:\n",
    "        y_true = np.array(y_true)\n",
    "    if type(y_pred) == list:\n",
    "        y_pred = np.array(y_pred)\n",
    "        \n",
    "    return np.sum((y_true != num_label) & (y_pred == num_label))\n",
    "\n",
    "def false_negatives(y_true, y_pred, num_label=1):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    if type(y_true) == list:\n",
    "        y_true = np.array(y_true)\n",
    "    if type(y_pred) == list:\n",
    "        y_pred = np.array(y_pred)\n",
    "        \n",
    "    return np.sum((y_true == num_label) & (y_pred != num_label))\n",
    "\n",
    "def true_negatives(y_true, y_pred, num_label=1):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    if type(y_true) == list:\n",
    "        y_true = np.array(y_true)\n",
    "    if type(y_pred) == list:\n",
    "        y_pred = np.array(y_pred)\n",
    "        \n",
    "    return np.sum((y_true != num_label) & (y_pred != num_label))\n",
    "\n",
    "def root_mean_squared_error(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    return sklearn.metrics.mean_squared_error(y_true, y_pred, squared=False)\n",
    "\n",
    "\n",
    "# From https://scikit-learn.org/stable/modules/model_evaluation.html\n",
    "_metrics = {\n",
    "    'classification': {\n",
    "        'accuracy': sklearn.metrics.accuracy_score,\n",
    "        'balanced_accuracy':sklearn.metrics.balanced_accuracy_score,\n",
    "        'average_precision':sklearn.metrics.average_precision_score,\n",
    "        'brier_score':sklearn.metrics.brier_score_loss,\n",
    "        'f1':sklearn.metrics.f1_score,\n",
    "        'log_loss':sklearn.metrics.log_loss,\n",
    "        'precision':sklearn.metrics.precision_score,\n",
    "        'recall':sklearn.metrics.recall_score,\n",
    "        'jaccard':sklearn.metrics.jaccard_score,\n",
    "        'matthews_corrcoef':sklearn.metrics.matthews_corrcoef,\n",
    "        'roc_auc':sklearn.metrics.roc_auc_score,\n",
    "        'true_positives':true_positives,\n",
    "        'TP':true_positives,\n",
    "        'false_positives':false_positives,\n",
    "        'FP':false_positives,\n",
    "        'false_negatives':false_negatives,\n",
    "        'FN':false_negatives,\n",
    "        'true_negatives':true_negatives,\n",
    "        'TN':true_negatives,\n",
    "        'confusion_matrix':sklearn.metrics.confusion_matrix\n",
    "    },\n",
    "    'regression':{\n",
    "        'explained_variance': sklearn.metrics.explained_variance_score,\n",
    "        'max_error': sklearn.metrics.max_error,\n",
    "        'mean_absolute_error': sklearn.metrics.mean_absolute_error,\n",
    "        'MAE': sklearn.metrics.mean_absolute_error,\n",
    "        'mean_squared_error': sklearn.metrics.mean_squared_error,\n",
    "        'MSE': sklearn.metrics.mean_squared_error,\n",
    "        'root_mean_squared_error': root_mean_squared_error,\n",
    "        'RMSE': root_mean_squared_error,\n",
    "        'mean_squared_log_error': sklearn.metrics.mean_squared_log_error,\n",
    "        'median_absolute_error': sklearn.metrics.median_absolute_error,\n",
    "        'r2': sklearn.metrics.r2_score,\n",
    "        'mean_poisson_deviance': sklearn.metrics.mean_poisson_deviance,\n",
    "        'mean_gamma_deviance': sklearn.metrics.mean_gamma_deviance\n",
    "    }\n",
    "} \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2 0 0]\n",
      " [0 0 1]\n",
      " [1 0 2]]\n",
      "2\n",
      "1\n",
      "1\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "y_true = [2, 0, 2, 2, 0, 1]\n",
    "y_pred = [0, 0, 2, 2, 0, 2]\n",
    "\n",
    "i=2\n",
    "print(sklearn.metrics.confusion_matrix(y_true,y_pred))\n",
    "print(true_positives(y_true,y_pred,num_label=i))\n",
    "print(false_positives(y_true,y_pred,num_label=i))\n",
    "print(false_negatives(y_true,y_pred,num_label=i))\n",
    "print(true_negatives(y_true,y_pred,num_label=i))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.625\n",
      "balanced_accuracy 0.6333333333333333\n",
      "average_precision 0.7\n",
      "brier_score 0.375\n",
      "f1 0.6666666666666665\n",
      "log_loss 12.95214109777028\n",
      "precision 0.75\n",
      "recall 0.6\n",
      "jaccard 0.5\n",
      "matthews_corrcoef 0.2581988897471611\n",
      "roc_auc 0.6333333333333334\n",
      "true_positives 3\n",
      "TP 3\n",
      "false_positives 1\n",
      "FP 1\n",
      "false_negatives 2\n",
      "FN 2\n",
      "true_negatives 2\n",
      "TN 2\n",
      "confusion_matrix [[2 1]\n",
      " [2 3]]\n"
     ]
    }
   ],
   "source": [
    "y_true = [0, 0, 0, 1, 1, 1, 1, 1]\n",
    "y_pred = [0, 1, 0, 1, 0, 1, 0, 1]\n",
    "\n",
    "for name, metric in metrics['classification'].items():\n",
    "    print(name, metric(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "explained_variance 0.8029556650246306\n",
      "max_error 1.5\n",
      "mean_absolute_error 0.75\n",
      "mean_squared_error 0.875\n",
      "root_mean_squared_error 0.9354143466934853\n",
      "mean_squared_log_error 0.03973012298459379\n",
      "median_absolute_error 0.75\n",
      "r2 0.7241379310344828\n",
      "mean_poisson_deviance 0.21861792444793338\n",
      "mean_gamma_deviance 0.06060673253815174\n"
     ]
    }
   ],
   "source": [
    "y_true = [3, 5, 2.5, 7]\n",
    "y_pred = [2.5, 5, 4, 8]\n",
    "for name, metric in metrics['regression'].items():\n",
    "    print(name, metric(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.625, 'f1': 0.6666666666666665, 'custom_1': 1}\n",
      "{'accuracy': 0.75, 'f1': 0.7499999999999999, 'custom_1': 1}\n",
      "{'RMSE': 0.9354143466934853, 'MAE': 0.75, 'custom_1': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:22: Warning: lol function not found\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "\n",
    "def compute_metrics(y_true, y_pred, metrics, task='classification'):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    if type(metrics) != list:\n",
    "        raise TypeError('metrics must be a list')\n",
    "        \n",
    "    if type(y_true) == list:\n",
    "        y_true = np.array(y_true)\n",
    "    if type(y_pred) == list:\n",
    "        y_pred = np.array(y_pred)\n",
    "        \n",
    "    fn_dict = {}\n",
    "    cnt_custom = 1\n",
    "\n",
    "    for fn in metrics:\n",
    "        if type(fn) == str:\n",
    "            if fn in _metrics[task]:\n",
    "                fn_dict[fn] = _metrics[task][fn]\n",
    "            else:\n",
    "                warnings.warn('%s function not found' % fn, Warning)\n",
    "        else:\n",
    "            fn_dict['custom_'+str(cnt_custom)] = fn\n",
    "            cnt_custom += 1\n",
    "    metrics = fn_dict\n",
    "    \n",
    "    if len(metrics.keys()) == 0:\n",
    "        raise ValueError('No valid metrics found')\n",
    "                    \n",
    "    if task == 'classification':\n",
    "        y_score = y_pred\n",
    "        y_pred = np.round(y_pred, 0)\n",
    "    \n",
    "    res = {}\n",
    "    for name, fn in metrics.items():\n",
    "        if (task == 'classification') & (\n",
    "            name in ['average_precision','roc_auc']):\n",
    "            \n",
    "            res[name] = fn(y_true, y_score)\n",
    "            continue\n",
    "        \n",
    "        res[name] = fn(y_true, y_pred)\n",
    "        \n",
    "    return res\n",
    "\n",
    "y_true = [0, 0, 0, 1, 1, 1, 1, 1]\n",
    "y_pred = [0, 1, 0, 1, 0, 1, 0, 1]\n",
    "metrics = ['accuracy','f1','lol',lambda y_true, y_pred:1]\n",
    "\n",
    "print(compute_metrics(y_true, y_pred, metrics))\n",
    "\n",
    "\n",
    "y_true = [0, 0, 0, 1, 1, 1, 1, 1]\n",
    "y_pred = [0.2, 0.49, 0, 1, 0.3, 0.8, 0, 1]\n",
    "\n",
    "print(compute_metrics(y_true, y_pred, metrics))\n",
    "\n",
    "y_true = [3, 5, 2.5, 7]\n",
    "y_pred = [2.5, 5, 4, 8]\n",
    "metrics = ['RMSE','MAE','lol',lambda y_true, y_pred:1]\n",
    "\n",
    "print(compute_metrics(y_true, y_pred, metrics, task='regression'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred accuracy\n",
      "pred balanced_accuracy\n",
      "score average_precision\n",
      "pred f1\n",
      "pred log_loss\n",
      "pred precision\n",
      "pred recall\n",
      "pred jaccard\n",
      "pred matthews_corrcoef\n",
      "score roc_auc\n",
      "pred true_positives\n",
      "pred TP\n",
      "pred false_positives\n",
      "pred FP\n",
      "pred false_negatives\n",
      "pred FN\n",
      "pred true_negatives\n",
      "pred TN\n",
      "pred confusion_matrix\n"
     ]
    }
   ],
   "source": [
    "for n, fn in _metrics['classification'].items():\n",
    "    h = str(fn.__code__.co_varnames)\n",
    "    if 'y_score' in h:\n",
    "        print('score', n)\n",
    "    if 'y_pred' in h:\n",
    "        print('pred', n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = [0, 0, 0, 1, 1, 1, 1, 1]\n",
    "y_pred = [0.2, 0.9, 0, 1, 0.3, 0.8, 0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
